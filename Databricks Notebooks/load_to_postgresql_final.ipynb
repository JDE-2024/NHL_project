{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a16db2ff-4705-4d43-9f93-305fbf1994c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 1 : Define JDBC and connection properties for Az PostgreSQL DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd75bee2-e107-41f0-b1cc-2b6d2b91c905",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define Java Database Connectivity (JDBC-industry-standard spec for accessing database management systems) URL for azure postgresql\n",
    "jdbc_url = \"jdbc:postgresql://nhlfinaldbserver.postgres.database.azure.com:5432/postgres\"\n",
    "\n",
    "# Define connection properties\n",
    "connection_properties = {\n",
    "    \"user\": \"nhladmin\",\n",
    "    \"password\": \"***\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c66746cf-9d8c-4570-9891-3c0d00b05a78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|?column?|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n",
      "Connection to PostgreSQL database successful!\n"
     ]
    }
   ],
   "source": [
    "# Test the connection with a basic query\n",
    "try:\n",
    "    # Use a simple query that doesn't rely on any existing tables\n",
    "    query = \"(SELECT 1) AS test_query\"\n",
    "    \n",
    "    # Load the result of the query into a DataFrame\n",
    "    testing123_df = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_properties)\n",
    "    \n",
    "    # Show the result to verify the connection\n",
    "    testing123_df.show()\n",
    "    \n",
    "    print(\"Connection to PostgreSQL database successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to PostgreSQL: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03e2c6ee-028b-43a9-90fb-1698e3f19d0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 2: Define dictionary of folder and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "216bdb7d-3fce-4d50-9d01-36e448bcb2cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define folder paths and corresponding table names\n",
    "folders_and_tables = {\n",
    "    \"gold/game_goalie_stats\": \"game_goalie_stats\",\n",
    "    \"gold/player_info\": \"player_info\",\n",
    "    \"gold/game_skater_stats\": \"game_skater_stats\",\n",
    "    \"gold/team_info\": \"team_info\",\n",
    "    \"gold/game_plays_xy\": \"game_plays\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7da555ad-79f6-433c-8167-97df6aff5d26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 3: Loop through dictionary to upload parquet to spark dataframe and then to postgresql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a22205-f62e-4b2c-a9f8-53b58b920de6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully written data from gold/game_goalie_stats to game_goalie_stats in PostgreSQL.\n",
      "Successfully written data from gold/player_info to player_info in PostgreSQL.\n",
      "Successfully written data from gold/game_skater_stats to game_skater_stats in PostgreSQL.\n",
      "Successfully written data from gold/team_info to team_info in PostgreSQL.\n",
      "Successfully written data from gold/game_plays_xy to game_plays in PostgreSQL.\n",
      "Data loading completed!\n"
     ]
    }
   ],
   "source": [
    "# Read Delta Parquet Files from Each Folder\n",
    "for folder, table_name in folders_and_tables.items():\n",
    "    folder_path = f\"/mnt/nhl-finalproject/{folder}\"  \n",
    "    \n",
    "        \n",
    "    # Load all Delta Parquet files in the folder into a single DataFrame\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(folder_path)\n",
    "        num_rows =df.count()\n",
    "        # Check if DataFrame is not empty before writing\n",
    "        if num_rows == 0:  # Check if the DataFrame is empty\n",
    "            print(f\"No data found in {folder}. Skipping write to {table_name}.\")\n",
    "            continue\n",
    "\n",
    "               \n",
    "        if num_rows > 100000:  \n",
    "            batched_df = df.repartition(5) # repartition if more than 100000\n",
    "        \n",
    "        else:\n",
    "            batched_df = df  # Keep it as is if it's small\n",
    "\n",
    "        # Write DataFrames to corresponding Azure PostgreSQL Tables\n",
    "        batched_df.write \\\n",
    "           .jdbc(url=jdbc_url, table=table_name, mode='overwrite', properties=connection_properties)\n",
    "        \n",
    "        print(f\"Successfully written data from {folder} to {table_name} in PostgreSQL.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write data from {folder} to {table_name}: {str(e)}\")\n",
    "\n",
    "print(\"Data loading completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_to_postgresql_final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
